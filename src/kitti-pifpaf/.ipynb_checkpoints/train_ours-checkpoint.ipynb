{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import skimage.transform\n",
    "import matplotlib.pyplot as plt\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "import json\n",
    "import cv2\n",
    "import copy as cp\n",
    "import math\n",
    "from PIL import Image\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pickle\n",
    "import random\n",
    "import multi_loss_model as mlmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '../../../pedestrianDepth-baseline/MonoDepth-PyTorch/data/test/image_2/image_02/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_data_ours_kitti.pickle', 'rb') as handle:\n",
    "    train_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('val_data_ours_kitti.pickle', 'rb') as handle:\n",
    "    test_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for person in train_data:\n",
    "    person['keypoints'] = np.array(person['keypoints']).reshape(17,2)\n",
    "for person in test_data:\n",
    "    person['keypoints'] = np.array(person['keypoints']).reshape(17,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(pedestrian):\n",
    "    im = np.array(Image.open(datapath+pedestrian['image_id']+'.png'), dtype=np.uint8)\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig,ax = plt.subplots(1, 2,figsize=(15,3.5))\n",
    "\n",
    "    # Display the image\n",
    "\n",
    "    box = pedestrian['gt_bbox']\n",
    "\n",
    "    box = list(map(lambda x: 0 if x<0 else x, box))\n",
    "    ax[0].imshow(im[int(box[1]):int(box[3]),int(box[0]):int(box[2])])\n",
    "    ax[1].imshow(im)\n",
    "\n",
    "    kp = np.array(pedestrian['keypoints']).reshape((17,2))\n",
    "\n",
    "    centered_kp=kp-[int(box[0]),int(box[1])]\n",
    "    rect1 = patches.Rectangle((box[0],box[1]),box[2]-box[0],box[3]-box[1],linewidth=2,edgecolor='r',facecolor='none')\n",
    "\n",
    "    box = pedestrian['pp_box']\n",
    "    rect2 = patches.Rectangle((box[0],box[1]),box[2]-box[0],box[3]-box[1],linewidth=2,edgecolor='g',facecolor='none')\n",
    "\n",
    "\n",
    "    ax[0].scatter(centered_kp[:,0],centered_kp[:,1],c='r')\n",
    "    # Add the patch to the Axes\n",
    "    ax[1].add_patch(rect1)\n",
    "    ax[1].add_patch(rect2)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count()>1:\n",
    "    #torch.cuda.device(1)\n",
    "    torch.cuda.set_device(0)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_orientation(angle):\n",
    "    if angle<0:\n",
    "        return 360+angle\n",
    "    if angle>360:\n",
    "        return angle-360\n",
    "    else:\n",
    "        return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_data:\n",
    "    data['gt_orient']=fix_orientation(data['gt_orient'])\n",
    "for data in test_data:\n",
    "    data['gt_orient']=fix_orientation(data['gt_orient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(train_data)\n",
    "random.shuffle(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_inputs_labels(pedestrians):\n",
    "    inputs=[]\n",
    "\n",
    "    angles=[]\n",
    "    sin_cos=[]\n",
    "    dimensions=[]\n",
    "    positions=[]\n",
    "    \n",
    "    for pedestrian in pedestrians:\n",
    "        angle=pedestrian['gt_orient']\n",
    "        angles.append(angle)\n",
    "        angle=angle*math.pi/180\n",
    "        inputs.append(pedestrian['keypoints'].flatten())\n",
    "        sin_cos.append([math.sin(angle),math.cos(angle)])\n",
    "        dimensions.append(pedestrian['gt_dim'])\n",
    "        positions.append(pedestrian['gt_pos'])\n",
    "    inputs=np.array(inputs)\n",
    "    sin_cos=np.array(sin_cos)\n",
    "    angles=np.array(angles)\n",
    "    dimensions=np.array(dimensions)\n",
    "    positions=np.array(positions)\n",
    "    return inputs, angles, sin_cos, dimensions, positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "[train_inputs, train_angles, train_sin_cos, train_dim, train_pos] = find_inputs_labels(train_data)\n",
    "[test_inputs, test_angles, test_sin_cos, test_dim, test_pos] = find_inputs_labels(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (1687, 34) (1687,) (1687, 2) (1687, 3) (1687, 3)\n",
      "test shape: (1798, 34) (1798,) (1798, 2) (1798, 3) (1798, 3)\n"
     ]
    }
   ],
   "source": [
    "print('train shape:',train_inputs.shape, train_angles.shape, train_sin_cos.shape, train_dim.shape, train_pos.shape)\n",
    "print('test shape:',test_inputs.shape, test_angles.shape, test_sin_cos.shape, test_dim.shape, test_pos.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = mlmodel.LinearModel(34, 8)\n",
    "net=net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sin2angle(data):\n",
    "    angles=[]\n",
    "    for angle in data:\n",
    "        angles.append(math.atan2(angle[0],angle[1])*180/math.pi)\n",
    "    return np.array(angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(inputs, model):\n",
    "    inputs = torch.from_numpy(inputs).float().to(device)\n",
    "    outputs = model(inputs)\n",
    "    [orientations, dimensions, positions] = (outputs[:,:2], outputs[:,2:5],\n",
    "                                             outputs[:,5:8])\n",
    "    [orientations, dimensions, positions] = (orientations.cpu().detach().numpy(),\n",
    "                                                      dimensions.cpu().detach().numpy(),\n",
    "                                                      positions.cpu().detach().numpy())\n",
    "    orientations = convert_sin2angle(orientations)\n",
    "    orientations = np.array(list(map(fix_orientation, orientations)))\n",
    "    \n",
    "    return orientations, dimensions, positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_losses(inputs, gt_angles, gt_dimensions, gt_positions, model):\n",
    "    \n",
    "    inputs=torch.from_numpy(inputs).float().to(device)\n",
    "    outputs = model(inputs)\n",
    "    [orientations, dimensions, positions] = (outputs[:,:2], outputs[:,2:5],\n",
    "                                             outputs[:,5:8])\n",
    "    [orientations, dimensions, positions] = (orientations.cpu().detach().numpy(),\n",
    "                                                      dimensions.cpu().detach().numpy(),\n",
    "                                                      positions.cpu().detach().numpy())\n",
    "    \n",
    "    orientations = convert_sin2angle(orientations)\n",
    "    pred_angles = []\n",
    "    for angle in orientations:\n",
    "        pred_angles.append(fix_orientation(angle))\n",
    "    pred_angles = np.array(pred_angles)\n",
    "    angle_loss = abs(gt_angles-pred_angles)\n",
    "    angle_loss = np.array(list(map(lambda x: x if x < 180 else 360-x, angle_loss)))\n",
    "    \n",
    "    dim_loss = np.linalg.norm(gt_dimensions - dimensions, axis=1)\n",
    "    \n",
    "    pos_loss = np.linalg.norm(gt_positions - positions, axis=1)\n",
    "    \n",
    "    return pred_angles, angle_loss, dim_loss, pos_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareData(Dataset):\n",
    "\n",
    "    def __init__(self, X, orientations, dimensions, positions):\n",
    "        if not torch.is_tensor(X):\n",
    "            self.X = torch.from_numpy(X).float()\n",
    "        \n",
    "        if not torch.is_tensor(orientations):\n",
    "            self.orientations = torch.from_numpy(orientations).float()\n",
    "        \n",
    "        if not torch.is_tensor(dimensions):\n",
    "            self.dimensions = torch.from_numpy(dimensions).float()\n",
    "        \n",
    "        if not torch.is_tensor(positions):\n",
    "            self.positions = torch.from_numpy(positions).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.orientations[idx], self.dimensions[idx], self.positions[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_cos_LossFunc = nn.L1Loss()\n",
    "dim_LossFunc = nn.MSELoss()\n",
    "pos_LossFunc = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.001\n",
    "    drop = 0.8\n",
    "    epochs_drop = 100.0\n",
    "    lrate = initial_lrate * math.pow(drop,\n",
    "                                     math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7985dcb668>]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(map(step_decay, range(1000))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_batches=100\n",
    "ds = PrepareData(X=train_inputs, orientations=train_sin_cos, dimensions=train_dim,\n",
    "                 positions=train_pos)\n",
    "\n",
    "ds = DataLoader(ds, batch_size=size_batches, shuffle=True)\n",
    "\n",
    "train_losses = {'angle loss':[], 'dim loss':[], 'pos loss':[]}\n",
    "test_losses = {'angle loss':[], 'dim loss':[], 'pos loss':[]}\n",
    "l1, l2, l3 = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[183.07710518  32.2893541    0.28266988   1.80504499]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(find_losses(test_inputs, test_angles, test_dim, test_pos, net), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1, p2, p3 = 1, 12, 0.045"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t train loss 10.31 0.14 1.16 \t test loss 32.88 0.29 1.84\n",
      "1 \t train loss 10.18 0.12 1.13 \t test loss 32.73 0.29 1.92\n",
      "2 \t train loss 11.13 0.14 1.19 \t test loss 31.7 0.32 1.77\n",
      "3 \t train loss 11.39 0.12 1.18 \t test loss 34.28 0.28 1.85\n",
      "4 \t train loss 10.63 0.13 1.29 \t test loss 32.5 0.28 1.76\n",
      "5 \t train loss 10.1 0.13 1.19 \t test loss 33.17 0.28 1.99\n",
      "6 \t train loss 10.45 0.13 1.21 \t test loss 33.02 0.3 1.86\n",
      "7 \t train loss 10.65 0.13 1.24 \t test loss 32.88 0.29 1.81\n",
      "8 \t train loss 11.29 0.15 1.31 \t test loss 34.89 0.32 1.84\n",
      "9 \t train loss 10.76 0.14 1.24 \t test loss 34.71 0.31 1.86\n",
      "10 \t train loss 10.38 0.13 1.2 \t test loss 34.82 0.29 1.91\n",
      "11 \t train loss 11.47 0.13 1.21 \t test loss 33.38 0.29 1.93\n",
      "12 \t train loss 11.25 0.13 1.26 \t test loss 33.31 0.28 1.88\n",
      "13 \t train loss 10.39 0.13 1.18 \t test loss 32.11 0.3 1.84\n",
      "14 \t train loss 10.39 0.15 1.16 \t test loss 33.36 0.31 1.8\n",
      "15 \t train loss 10.52 0.13 1.21 \t test loss 33.07 0.29 1.85\n",
      "16 \t train loss 10.27 0.13 1.23 \t test loss 32.38 0.28 1.84\n",
      "17 \t train loss 10.91 0.13 1.23 \t test loss 33.34 0.29 1.88\n",
      "18 \t train loss 10.64 0.14 1.31 \t test loss 32.98 0.3 1.87\n",
      "19 \t train loss 11.12 0.13 1.23 \t test loss 33.93 0.28 1.86\n",
      "20 \t train loss 11.16 0.13 1.2 \t test loss 33.51 0.29 1.98\n",
      "21 \t train loss 10.62 0.13 1.2 \t test loss 32.22 0.29 1.93\n",
      "22 \t train loss 10.82 0.13 1.21 \t test loss 31.59 0.29 1.9\n",
      "23 \t train loss 11.09 0.13 1.35 \t test loss 33.21 0.29 1.82\n",
      "24 \t train loss 10.6 0.13 1.25 \t test loss 32.18 0.3 1.91\n",
      "25 \t train loss 11.11 0.14 1.26 \t test loss 33.56 0.29 2.04\n",
      "26 \t train loss 11.21 0.14 1.2 \t test loss 32.06 0.28 1.84\n",
      "27 \t train loss 10.94 0.13 1.22 \t test loss 33.75 0.29 1.86\n",
      "28 \t train loss 11.4 0.13 1.29 \t test loss 33.33 0.3 1.83\n",
      "29 \t train loss 10.35 0.14 1.23 \t test loss 31.82 0.3 1.92\n",
      "30 \t train loss 10.91 0.13 1.28 \t test loss 35.46 0.27 1.8\n",
      "31 \t train loss 11.77 0.14 1.25 \t test loss 33.14 0.29 1.89\n",
      "32 \t train loss 10.43 0.13 1.25 \t test loss 32.98 0.28 1.88\n",
      "33 \t train loss 11.08 0.14 1.18 \t test loss 33.0 0.3 1.98\n",
      "34 \t train loss 10.35 0.13 1.2 \t test loss 32.64 0.3 2.02\n",
      "35 \t train loss 10.79 0.13 1.2 \t test loss 32.53 0.29 1.93\n",
      "36 \t train loss 11.25 0.13 1.19 \t test loss 33.95 0.29 1.84\n",
      "37 \t train loss 10.35 0.15 1.26 \t test loss 33.6 0.28 2.04\n",
      "38 \t train loss 10.4 0.15 1.22 \t test loss 32.79 0.3 1.97\n",
      "39 \t train loss 11.82 0.14 1.23 \t test loss 34.7 0.3 1.94\n",
      "40 \t train loss 11.23 0.14 1.35 \t test loss 33.19 0.29 1.9\n",
      "41 \t train loss 11.34 0.14 1.26 \t test loss 34.38 0.29 2.03\n",
      "42 \t train loss 11.45 0.13 1.23 \t test loss 33.27 0.29 1.9\n",
      "43 \t train loss 11.12 0.15 1.27 \t test loss 33.78 0.31 1.83\n",
      "44 \t train loss 10.35 0.13 1.25 \t test loss 33.29 0.29 1.95\n",
      "45 \t train loss 10.45 0.13 1.22 \t test loss 32.68 0.3 1.9\n",
      "46 \t train loss 10.73 0.14 1.25 \t test loss 33.96 0.31 1.89\n",
      "47 \t train loss 11.02 0.13 1.23 \t test loss 33.53 0.29 1.89\n",
      "48 \t train loss 10.31 0.13 1.3 \t test loss 33.61 0.29 2.02\n",
      "49 \t train loss 12.08 0.13 1.27 \t test loss 34.34 0.3 1.9\n",
      "50 \t train loss 10.77 0.13 1.22 \t test loss 32.62 0.3 1.9\n",
      "51 \t train loss 10.64 0.13 1.2 \t test loss 33.73 0.28 1.92\n",
      "52 \t train loss 10.8 0.13 1.16 \t test loss 33.46 0.29 2.0\n",
      "53 \t train loss 10.41 0.13 1.26 \t test loss 32.43 0.29 1.79\n",
      "54 \t train loss 10.89 0.14 1.26 \t test loss 34.88 0.3 1.87\n",
      "55 \t train loss 11.58 0.14 1.44 \t test loss 34.0 0.3 2.19\n",
      "56 \t train loss 11.0 0.14 1.16 \t test loss 33.49 0.29 1.85\n",
      "57 \t train loss 10.47 0.13 1.31 \t test loss 32.33 0.29 1.88\n",
      "58 \t train loss 10.64 0.13 1.24 \t test loss 34.4 0.29 1.77\n",
      "59 \t train loss 11.14 0.13 1.36 \t test loss 35.13 0.28 1.87\n",
      "60 \t train loss 11.04 0.13 1.21 \t test loss 33.52 0.28 1.94\n",
      "61 \t train loss 10.55 0.13 1.24 \t test loss 34.83 0.29 1.97\n",
      "62 \t train loss 11.15 0.14 1.21 \t test loss 32.83 0.29 1.82\n",
      "63 \t train loss 11.24 0.13 1.4 \t test loss 33.8 0.29 1.88\n",
      "64 \t train loss 11.01 0.13 1.21 \t test loss 35.03 0.28 1.84\n",
      "65 \t train loss 10.33 0.13 1.22 \t test loss 33.91 0.28 1.94\n",
      "66 \t train loss 11.14 0.13 1.33 \t test loss 33.81 0.3 1.78\n",
      "67 \t train loss 10.78 0.13 1.35 \t test loss 33.6 0.29 1.83\n",
      "68 \t train loss 11.43 0.13 1.22 \t test loss 34.52 0.28 1.96\n",
      "69 \t train loss 10.98 0.14 1.22 \t test loss 33.42 0.29 1.89\n",
      "70 \t train loss 10.26 0.15 1.21 \t test loss 32.79 0.31 1.95\n",
      "71 \t train loss 10.6 0.13 1.26 \t test loss 32.43 0.28 1.78\n",
      "72 \t train loss 10.48 0.13 1.31 \t test loss 33.39 0.29 2.04\n",
      "73 \t train loss 10.87 0.14 1.18 \t test loss 33.28 0.28 1.91\n",
      "74 \t train loss 10.78 0.13 1.24 \t test loss 32.5 0.28 1.82\n",
      "75 \t train loss 10.64 0.13 1.24 \t test loss 33.54 0.29 1.89\n",
      "76 \t train loss 11.07 0.13 1.24 \t test loss 34.43 0.28 1.82\n",
      "77 \t train loss 11.11 0.13 1.27 \t test loss 32.84 0.29 1.79\n",
      "78 \t train loss 10.47 0.13 1.21 \t test loss 32.87 0.29 1.9\n",
      "79 \t train loss 11.83 0.15 1.29 \t test loss 34.17 0.3 2.03\n",
      "80 \t train loss 10.26 0.13 1.39 \t test loss 32.7 0.28 1.85\n",
      "81 \t train loss 10.48 0.13 1.23 \t test loss 32.63 0.28 1.98\n",
      "82 \t train loss 10.5 0.14 1.24 \t test loss 32.5 0.27 1.92\n",
      "83 \t train loss 10.37 0.13 1.2 \t test loss 32.84 0.27 1.87\n",
      "84 \t train loss 10.75 0.13 1.23 \t test loss 32.71 0.3 1.89\n",
      "85 \t train loss 11.51 0.14 1.25 \t test loss 34.15 0.3 1.99\n",
      "86 \t train loss 10.23 0.14 1.25 \t test loss 34.19 0.3 1.96\n",
      "87 \t train loss 10.29 0.13 1.24 \t test loss 33.48 0.29 1.89\n",
      "88 \t train loss 10.86 0.13 1.25 \t test loss 34.99 0.3 1.9\n",
      "89 \t train loss 10.03 0.13 1.2 \t test loss 33.32 0.28 1.94\n",
      "90 \t train loss 11.32 0.14 1.29 \t test loss 33.05 0.29 1.97\n",
      "91 \t train loss 11.75 0.14 1.23 \t test loss 31.88 0.28 1.93\n",
      "92 \t train loss 11.16 0.13 1.29 \t test loss 33.44 0.3 1.92\n",
      "93 \t train loss 10.83 0.15 1.29 \t test loss 33.87 0.31 1.88\n",
      "94 \t train loss 11.52 0.13 1.27 \t test loss 33.02 0.3 1.91\n",
      "95 \t train loss 10.89 0.14 1.22 \t test loss 33.92 0.29 1.98\n",
      "96 \t train loss 11.11 0.15 1.21 \t test loss 34.06 0.31 1.96\n",
      "97 \t train loss 10.76 0.13 1.17 \t test loss 33.23 0.28 1.88\n",
      "98 \t train loss 10.78 0.13 1.29 \t test loss 34.93 0.28 1.84\n",
      "99 \t train loss 10.58 0.13 1.2 \t test loss 33.93 0.29 1.87\n",
      "100 \t train loss 10.14 0.12 1.29 \t test loss 32.11 0.28 1.82\n",
      "101 \t train loss 9.89 0.12 1.17 \t test loss 33.3 0.29 1.97\n",
      "102 \t train loss 10.7 0.13 1.23 \t test loss 32.9 0.29 1.9\n",
      "103 \t train loss 11.2 0.13 1.2 \t test loss 34.49 0.3 1.88\n",
      "104 \t train loss 10.18 0.12 1.2 \t test loss 34.22 0.29 1.87\n",
      "105 \t train loss 10.64 0.13 1.2 \t test loss 32.78 0.3 1.9\n",
      "106 \t train loss 10.34 0.14 1.2 \t test loss 33.14 0.29 1.84\n",
      "107 \t train loss 10.0 0.12 1.23 \t test loss 33.54 0.28 1.98\n",
      "108 \t train loss 10.38 0.12 1.19 \t test loss 33.97 0.29 1.91\n",
      "109 \t train loss 10.08 0.12 1.18 \t test loss 32.7 0.29 1.84\n",
      "110 \t train loss 9.83 0.14 1.31 \t test loss 32.36 0.3 1.82\n",
      "111 \t train loss 10.11 0.12 1.17 \t test loss 32.61 0.28 1.92\n",
      "112 \t train loss 9.81 0.13 1.19 \t test loss 32.56 0.28 1.86\n",
      "113 \t train loss 10.52 0.13 1.22 \t test loss 34.16 0.28 1.82\n",
      "114 \t train loss 10.83 0.14 1.19 \t test loss 32.62 0.3 1.79\n",
      "115 \t train loss 9.51 0.12 1.14 \t test loss 33.31 0.29 1.85\n",
      "116 \t train loss 10.6 0.13 1.23 \t test loss 33.66 0.28 1.98\n",
      "117 \t train loss 10.16 0.12 1.2 \t test loss 33.96 0.29 1.77\n",
      "118 \t train loss 9.61 0.12 1.23 \t test loss 33.54 0.29 2.03\n",
      "119 \t train loss 9.68 0.12 1.16 \t test loss 32.47 0.29 1.87\n",
      "120 \t train loss 10.12 0.12 1.25 \t test loss 32.39 0.28 1.85\n",
      "121 \t train loss 10.58 0.13 1.13 \t test loss 32.83 0.3 1.85\n",
      "122 \t train loss 10.11 0.12 1.2 \t test loss 33.93 0.28 1.89\n",
      "123 \t train loss 9.75 0.13 1.19 \t test loss 33.27 0.3 1.86\n",
      "124 \t train loss 9.96 0.13 1.17 \t test loss 33.88 0.3 1.87\n",
      "125 \t train loss 10.24 0.13 1.19 \t test loss 33.4 0.29 2.07\n",
      "126 \t train loss 10.18 0.12 1.34 \t test loss 33.9 0.28 1.81\n",
      "127 \t train loss 10.16 0.12 1.12 \t test loss 34.41 0.29 1.9\n",
      "128 \t train loss 10.16 0.13 1.23 \t test loss 34.19 0.3 1.78\n",
      "129 \t train loss 10.0 0.14 1.39 \t test loss 32.15 0.3 1.87\n",
      "130 \t train loss 10.69 0.14 1.26 \t test loss 35.03 0.3 1.82\n",
      "131 \t train loss 10.01 0.13 1.18 \t test loss 33.38 0.3 1.85\n",
      "132 \t train loss 9.44 0.12 1.17 \t test loss 32.88 0.28 1.88\n",
      "133 \t train loss 10.37 0.12 1.26 \t test loss 34.48 0.29 1.77\n",
      "134 \t train loss 9.62 0.12 1.14 \t test loss 32.85 0.29 1.89\n",
      "135 \t train loss 9.96 0.13 1.2 \t test loss 33.37 0.28 1.81\n",
      "136 \t train loss 9.93 0.12 1.36 \t test loss 33.73 0.28 1.86\n",
      "137 \t train loss 9.81 0.13 1.23 \t test loss 33.65 0.29 1.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 \t train loss 10.04 0.13 1.17 \t test loss 34.12 0.28 1.81\n",
      "139 \t train loss 9.79 0.12 1.22 \t test loss 33.72 0.29 1.74\n",
      "140 \t train loss 10.29 0.12 1.22 \t test loss 33.18 0.29 2.03\n",
      "141 \t train loss 9.9 0.12 1.17 \t test loss 33.38 0.28 1.89\n",
      "142 \t train loss 10.04 0.12 1.18 \t test loss 34.49 0.29 1.88\n",
      "143 \t train loss 9.91 0.13 1.14 \t test loss 34.55 0.3 1.91\n",
      "144 \t train loss 10.44 0.13 1.22 \t test loss 34.2 0.29 1.87\n",
      "145 \t train loss 9.82 0.12 1.19 \t test loss 33.26 0.29 1.88\n",
      "146 \t train loss 10.56 0.12 1.15 \t test loss 33.21 0.29 1.82\n",
      "147 \t train loss 9.62 0.12 1.16 \t test loss 32.45 0.28 1.83\n",
      "148 \t train loss 10.02 0.13 1.22 \t test loss 32.46 0.29 1.89\n",
      "149 \t train loss 9.45 0.12 1.17 \t test loss 33.31 0.28 1.8\n",
      "150 \t train loss 9.74 0.12 1.16 \t test loss 34.07 0.29 1.81\n",
      "151 \t train loss 10.87 0.14 1.21 \t test loss 34.6 0.3 1.85\n",
      "152 \t train loss 9.74 0.12 1.18 \t test loss 32.94 0.28 1.82\n",
      "153 \t train loss 9.65 0.13 1.29 \t test loss 33.08 0.29 1.78\n",
      "154 \t train loss 10.1 0.13 1.16 \t test loss 32.32 0.3 1.88\n",
      "155 \t train loss 9.77 0.14 1.19 \t test loss 33.88 0.31 1.82\n",
      "156 \t train loss 10.46 0.13 1.15 \t test loss 33.27 0.3 1.83\n",
      "157 \t train loss 9.88 0.12 1.15 \t test loss 33.85 0.29 1.92\n",
      "158 \t train loss 10.2 0.12 1.21 \t test loss 32.54 0.29 1.86\n",
      "159 \t train loss 10.07 0.13 1.19 \t test loss 33.37 0.29 1.91\n",
      "160 \t train loss 10.25 0.12 1.14 \t test loss 34.27 0.29 1.79\n",
      "161 \t train loss 10.13 0.12 1.21 \t test loss 33.45 0.29 1.83\n",
      "162 \t train loss 9.99 0.12 1.19 \t test loss 33.98 0.29 1.89\n",
      "163 \t train loss 10.43 0.13 1.19 \t test loss 34.72 0.31 1.92\n",
      "164 \t train loss 10.0 0.12 1.19 \t test loss 32.86 0.29 1.87\n",
      "165 \t train loss 9.94 0.12 1.15 \t test loss 34.21 0.29 1.91\n",
      "166 \t train loss 9.92 0.12 1.29 \t test loss 33.38 0.29 1.83\n",
      "167 \t train loss 10.32 0.13 1.18 \t test loss 34.5 0.31 1.93\n",
      "168 \t train loss 9.58 0.12 1.2 \t test loss 33.41 0.28 1.77\n",
      "169 \t train loss 10.01 0.12 1.22 \t test loss 33.36 0.29 1.94\n",
      "170 \t train loss 9.82 0.13 1.15 \t test loss 33.54 0.29 1.98\n",
      "171 \t train loss 9.27 0.13 1.17 \t test loss 32.58 0.3 1.91\n",
      "172 \t train loss 9.99 0.12 1.29 \t test loss 32.62 0.29 2.03\n",
      "173 \t train loss 9.86 0.13 1.23 \t test loss 32.29 0.3 1.81\n",
      "174 \t train loss 10.16 0.12 1.22 \t test loss 33.49 0.28 1.84\n",
      "175 \t train loss 9.62 0.13 1.21 \t test loss 34.33 0.29 1.87\n",
      "176 \t train loss 10.35 0.12 1.16 \t test loss 32.97 0.28 2.04\n",
      "177 \t train loss 10.2 0.12 1.2 \t test loss 33.6 0.29 1.86\n",
      "178 \t train loss 10.29 0.13 1.24 \t test loss 34.26 0.3 1.74\n",
      "179 \t train loss 9.64 0.12 1.2 \t test loss 33.47 0.3 1.76\n",
      "180 \t train loss 10.09 0.13 1.15 \t test loss 35.09 0.28 1.99\n",
      "181 \t train loss 9.86 0.12 1.2 \t test loss 33.97 0.28 1.9\n",
      "182 \t train loss 9.79 0.13 1.16 \t test loss 33.52 0.29 1.81\n",
      "183 \t train loss 10.67 0.12 1.2 \t test loss 33.69 0.29 1.88\n",
      "184 \t train loss 9.87 0.12 1.19 \t test loss 32.53 0.28 1.83\n",
      "185 \t train loss 9.85 0.13 1.13 \t test loss 32.44 0.3 1.89\n",
      "186 \t train loss 9.95 0.12 1.22 \t test loss 31.89 0.29 1.83\n",
      "187 \t train loss 9.57 0.12 1.21 \t test loss 34.49 0.29 1.82\n",
      "188 \t train loss 9.6 0.12 1.2 \t test loss 33.98 0.3 1.94\n",
      "189 \t train loss 9.74 0.12 1.14 \t test loss 32.86 0.29 1.95\n",
      "190 \t train loss 9.93 0.12 1.15 \t test loss 32.57 0.29 1.9\n",
      "191 \t train loss 9.85 0.12 1.17 \t test loss 33.11 0.29 1.88\n",
      "192 \t train loss 10.26 0.12 1.16 \t test loss 34.02 0.29 1.84\n",
      "193 \t train loss 10.54 0.14 1.28 \t test loss 32.97 0.3 1.8\n",
      "194 \t train loss 9.64 0.12 1.19 \t test loss 32.23 0.28 1.92\n",
      "195 \t train loss 9.55 0.12 1.22 \t test loss 33.82 0.28 1.87\n",
      "196 \t train loss 9.78 0.12 1.18 \t test loss 33.69 0.29 2.04\n",
      "197 \t train loss 9.95 0.12 1.15 \t test loss 34.9 0.29 1.91\n",
      "198 \t train loss 9.74 0.15 1.24 \t test loss 31.85 0.31 1.94\n",
      "199 \t train loss 9.53 0.12 1.13 \t test loss 33.23 0.28 1.9\n",
      "200 \t train loss 9.69 0.12 1.18 \t test loss 32.85 0.29 1.76\n",
      "201 \t train loss 9.35 0.12 1.12 \t test loss 31.29 0.29 1.78\n",
      "202 \t train loss 9.21 0.12 1.17 \t test loss 34.1 0.29 1.97\n",
      "203 \t train loss 9.54 0.12 1.19 \t test loss 32.89 0.28 1.84\n",
      "204 \t train loss 9.59 0.12 1.17 \t test loss 33.0 0.29 1.81\n",
      "205 \t train loss 9.41 0.12 1.11 \t test loss 34.08 0.28 1.9\n",
      "206 \t train loss 10.01 0.12 1.1 \t test loss 32.99 0.3 1.93\n",
      "207 \t train loss 9.79 0.12 1.16 \t test loss 31.79 0.29 1.8\n",
      "208 \t train loss 9.17 0.12 1.13 \t test loss 34.02 0.29 1.87\n",
      "209 \t train loss 9.48 0.12 1.23 \t test loss 32.73 0.29 1.87\n",
      "210 \t train loss 9.0 0.12 1.13 \t test loss 31.98 0.29 1.85\n",
      "211 \t train loss 9.63 0.12 1.15 \t test loss 33.09 0.29 1.85\n",
      "212 \t train loss 8.97 0.11 1.15 \t test loss 34.27 0.28 1.88\n",
      "213 \t train loss 9.49 0.11 1.13 \t test loss 33.96 0.29 1.87\n",
      "214 \t train loss 9.49 0.12 1.18 \t test loss 33.9 0.29 2.0\n",
      "215 \t train loss 9.18 0.12 1.13 \t test loss 32.61 0.28 1.82\n",
      "216 \t train loss 8.99 0.12 1.13 \t test loss 32.98 0.29 1.85\n",
      "217 \t train loss 9.18 0.12 1.18 \t test loss 32.08 0.28 1.79\n",
      "218 \t train loss 9.46 0.12 1.13 \t test loss 32.48 0.28 1.79\n",
      "219 \t train loss 9.5 0.11 1.19 \t test loss 32.32 0.28 1.86\n",
      "220 \t train loss 9.89 0.12 1.17 \t test loss 32.86 0.28 1.79\n",
      "221 \t train loss 8.65 0.12 1.16 \t test loss 32.64 0.28 1.78\n",
      "222 \t train loss 9.19 0.12 1.11 \t test loss 33.75 0.28 1.83\n",
      "223 \t train loss 9.11 0.13 1.17 \t test loss 32.98 0.3 1.78\n",
      "224 \t train loss 9.13 0.12 1.24 \t test loss 32.75 0.28 1.77\n",
      "225 \t train loss 9.56 0.12 1.13 \t test loss 34.44 0.29 1.86\n",
      "226 \t train loss 9.08 0.12 1.15 \t test loss 33.88 0.29 1.83\n",
      "227 \t train loss 10.05 0.12 1.27 \t test loss 34.43 0.29 1.81\n",
      "228 \t train loss 9.29 0.12 1.13 \t test loss 33.09 0.28 1.85\n",
      "229 \t train loss 9.29 0.11 1.11 \t test loss 32.28 0.29 1.8\n",
      "230 \t train loss 9.31 0.11 1.13 \t test loss 34.43 0.28 1.86\n",
      "231 \t train loss 9.83 0.12 1.15 \t test loss 33.71 0.29 1.77\n",
      "232 \t train loss 8.91 0.11 1.13 \t test loss 32.31 0.28 1.84\n",
      "233 \t train loss 8.93 0.12 1.18 \t test loss 33.78 0.28 1.85\n",
      "234 \t train loss 8.87 0.12 1.14 \t test loss 33.15 0.3 1.87\n",
      "235 \t train loss 9.34 0.12 1.22 \t test loss 32.8 0.28 1.8\n",
      "236 \t train loss 9.59 0.12 1.28 \t test loss 32.63 0.29 2.03\n",
      "237 \t train loss 8.96 0.12 1.13 \t test loss 31.65 0.29 1.86\n",
      "238 \t train loss 9.26 0.12 1.14 \t test loss 33.18 0.28 1.76\n",
      "239 \t train loss 8.99 0.11 1.12 \t test loss 32.96 0.29 1.8\n",
      "240 \t train loss 9.03 0.12 1.1 \t test loss 32.48 0.29 1.79\n",
      "241 \t train loss 9.89 0.12 1.15 \t test loss 35.02 0.29 1.85\n",
      "242 \t train loss 9.63 0.12 1.14 \t test loss 32.71 0.29 1.82\n",
      "243 \t train loss 9.41 0.12 1.17 \t test loss 33.87 0.3 1.86\n",
      "244 \t train loss 8.94 0.11 1.17 \t test loss 32.12 0.29 1.88\n",
      "245 \t train loss 9.29 0.11 1.14 \t test loss 32.91 0.28 1.82\n",
      "246 \t train loss 9.13 0.13 1.17 \t test loss 33.46 0.3 1.8\n",
      "247 \t train loss 8.8 0.12 1.16 \t test loss 32.56 0.29 1.96\n",
      "248 \t train loss 9.2 0.12 1.16 \t test loss 32.87 0.3 1.82\n",
      "249 \t train loss 9.05 0.12 1.13 \t test loss 33.85 0.3 1.89\n",
      "250 \t train loss 8.99 0.12 1.14 \t test loss 33.19 0.3 1.8\n",
      "251 \t train loss 8.95 0.12 1.11 \t test loss 34.19 0.28 1.79\n",
      "252 \t train loss 9.03 0.12 1.11 \t test loss 34.03 0.29 1.84\n",
      "253 \t train loss 9.04 0.11 1.17 \t test loss 33.4 0.29 1.81\n",
      "254 \t train loss 8.93 0.12 1.09 \t test loss 33.0 0.29 1.98\n",
      "255 \t train loss 9.85 0.12 1.3 \t test loss 34.08 0.29 1.77\n",
      "256 \t train loss 9.04 0.11 1.17 \t test loss 33.71 0.29 1.77\n",
      "257 \t train loss 8.84 0.12 1.14 \t test loss 33.26 0.29 1.82\n",
      "258 \t train loss 8.94 0.11 1.13 \t test loss 35.42 0.28 1.85\n",
      "259 \t train loss 8.83 0.12 1.15 \t test loss 32.96 0.29 1.85\n",
      "260 \t train loss 9.15 0.12 1.15 \t test loss 33.18 0.3 1.88\n",
      "261 \t train loss 9.12 0.11 1.12 \t test loss 32.9 0.28 1.83\n",
      "262 \t train loss 9.16 0.12 1.09 \t test loss 34.12 0.28 1.84\n",
      "263 \t train loss 9.16 0.12 1.18 \t test loss 33.54 0.29 1.82\n",
      "264 \t train loss 9.08 0.12 1.18 \t test loss 31.98 0.29 1.85\n",
      "265 \t train loss 9.1 0.12 1.13 \t test loss 32.88 0.28 1.78\n",
      "266 \t train loss 8.95 0.12 1.14 \t test loss 34.27 0.29 1.87\n",
      "267 \t train loss 8.72 0.12 1.2 \t test loss 32.44 0.29 1.78\n",
      "268 \t train loss 8.57 0.12 1.09 \t test loss 33.68 0.29 1.85\n",
      "269 \t train loss 8.64 0.11 1.27 \t test loss 32.55 0.28 1.85\n",
      "270 \t train loss 9.33 0.12 1.11 \t test loss 33.0 0.29 1.87\n",
      "271 \t train loss 8.73 0.11 1.14 \t test loss 32.42 0.29 1.93\n",
      "272 \t train loss 8.94 0.12 1.14 \t test loss 33.49 0.29 1.82\n",
      "273 \t train loss 9.2 0.11 1.15 \t test loss 31.34 0.28 1.92\n",
      "274 \t train loss 9.02 0.12 1.11 \t test loss 32.85 0.28 1.92\n",
      "275 \t train loss 8.95 0.11 1.21 \t test loss 33.35 0.29 1.87\n",
      "276 \t train loss 9.05 0.12 1.12 \t test loss 33.53 0.29 1.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277 \t train loss 8.29 0.12 1.17 \t test loss 32.33 0.29 1.87\n",
      "278 \t train loss 8.89 0.12 1.16 \t test loss 32.74 0.29 1.85\n",
      "279 \t train loss 9.23 0.11 1.16 \t test loss 33.63 0.28 1.82\n",
      "280 \t train loss 9.04 0.12 1.14 \t test loss 33.1 0.29 1.96\n",
      "281 \t train loss 8.88 0.12 1.12 \t test loss 32.49 0.3 1.81\n",
      "282 \t train loss 8.79 0.11 1.13 \t test loss 33.92 0.29 1.84\n",
      "283 \t train loss 8.93 0.12 1.23 \t test loss 33.83 0.28 1.83\n",
      "284 \t train loss 8.84 0.12 1.14 \t test loss 32.84 0.28 1.85\n",
      "285 \t train loss 8.72 0.11 1.09 \t test loss 33.71 0.29 1.89\n",
      "286 \t train loss 8.71 0.11 1.15 \t test loss 32.43 0.28 1.82\n",
      "287 \t train loss 8.87 0.12 1.14 \t test loss 32.45 0.28 1.87\n",
      "288 \t train loss 8.86 0.11 1.12 \t test loss 33.59 0.29 1.74\n",
      "289 \t train loss 8.86 0.11 1.15 \t test loss 32.64 0.29 1.85\n",
      "290 \t train loss 8.89 0.12 1.14 \t test loss 33.24 0.29 1.83\n",
      "291 \t train loss 8.79 0.11 1.11 \t test loss 33.11 0.28 1.79\n",
      "292 \t train loss 8.46 0.12 1.13 \t test loss 33.93 0.3 1.85\n",
      "293 \t train loss 8.82 0.11 1.21 \t test loss 32.03 0.28 1.79\n",
      "294 \t train loss 8.7 0.12 1.15 \t test loss 33.6 0.28 1.88\n",
      "295 \t train loss 8.75 0.12 1.16 \t test loss 32.56 0.29 1.79\n",
      "296 \t train loss 8.46 0.11 1.11 \t test loss 32.93 0.28 1.9\n",
      "297 \t train loss 8.93 0.12 1.15 \t test loss 33.23 0.29 1.86\n",
      "298 \t train loss 9.17 0.12 1.12 \t test loss 32.56 0.29 1.82\n",
      "299 \t train loss 8.75 0.11 1.11 \t test loss 32.24 0.28 1.84\n",
      "300 \t train loss 8.38 0.11 1.09 \t test loss 32.18 0.29 1.88\n",
      "301 \t train loss 9.13 0.13 1.21 \t test loss 32.01 0.3 1.8\n",
      "302 \t train loss 8.65 0.11 1.11 \t test loss 32.91 0.29 1.81\n",
      "303 \t train loss 9.0 0.12 1.13 \t test loss 33.23 0.28 1.79\n",
      "304 \t train loss 8.33 0.12 1.08 \t test loss 33.06 0.29 1.82\n",
      "305 \t train loss 8.62 0.11 1.1 \t test loss 34.21 0.28 1.89\n",
      "306 \t train loss 8.82 0.11 1.14 \t test loss 32.43 0.29 1.78\n",
      "307 \t train loss 8.73 0.13 1.16 \t test loss 32.0 0.3 1.74\n",
      "308 \t train loss 8.58 0.11 1.09 \t test loss 32.48 0.28 1.79\n",
      "309 \t train loss 8.49 0.11 1.09 \t test loss 33.83 0.29 1.85\n",
      "310 \t train loss 8.57 0.11 1.1 \t test loss 33.42 0.29 1.87\n",
      "311 \t train loss 8.54 0.11 1.13 \t test loss 32.29 0.29 1.81\n",
      "312 \t train loss 8.36 0.12 1.08 \t test loss 32.19 0.3 1.85\n",
      "313 \t train loss 8.78 0.11 1.17 \t test loss 31.86 0.28 1.78\n",
      "314 \t train loss 8.89 0.11 1.12 \t test loss 33.27 0.28 1.82\n",
      "315 \t train loss 8.28 0.11 1.15 \t test loss 32.51 0.29 1.92\n",
      "316 \t train loss 8.68 0.11 1.16 \t test loss 33.88 0.28 1.76\n",
      "317 \t train loss 8.62 0.11 1.09 \t test loss 32.48 0.28 1.83\n",
      "318 \t train loss 8.5 0.13 1.22 \t test loss 32.49 0.3 1.75\n",
      "319 \t train loss 8.66 0.11 1.12 \t test loss 31.75 0.28 1.9\n",
      "320 \t train loss 8.55 0.11 1.11 \t test loss 33.59 0.28 1.85\n",
      "321 \t train loss 8.47 0.11 1.07 \t test loss 34.03 0.28 1.82\n",
      "322 \t train loss 8.86 0.11 1.1 \t test loss 31.8 0.29 1.76\n",
      "323 \t train loss 8.53 0.12 1.18 \t test loss 33.13 0.29 1.8\n",
      "324 \t train loss 8.23 0.11 1.11 \t test loss 32.91 0.29 1.76\n",
      "325 \t train loss 8.75 0.11 1.12 \t test loss 32.48 0.28 1.9\n",
      "326 \t train loss 8.47 0.11 1.15 \t test loss 33.37 0.28 1.71\n",
      "327 \t train loss 8.47 0.11 1.12 \t test loss 33.06 0.28 1.76\n",
      "328 \t train loss 8.41 0.11 1.16 \t test loss 31.75 0.29 1.92\n",
      "329 \t train loss 8.08 0.11 1.19 \t test loss 32.98 0.28 1.76\n",
      "330 \t train loss 8.27 0.11 1.17 \t test loss 32.63 0.28 1.77\n",
      "331 \t train loss 8.71 0.11 1.07 \t test loss 34.09 0.29 1.81\n",
      "332 \t train loss 8.81 0.11 1.07 \t test loss 34.16 0.28 1.94\n",
      "333 \t train loss 8.33 0.11 1.07 \t test loss 32.24 0.29 1.83\n",
      "334 \t train loss 8.88 0.11 1.11 \t test loss 33.14 0.29 1.76\n",
      "335 \t train loss 8.05 0.11 1.13 \t test loss 33.08 0.29 1.81\n",
      "336 \t train loss 8.29 0.11 1.11 \t test loss 32.92 0.28 1.75\n",
      "337 \t train loss 7.94 0.12 1.14 \t test loss 32.72 0.3 1.78\n",
      "338 \t train loss 8.33 0.12 1.11 \t test loss 32.17 0.3 1.84\n",
      "339 \t train loss 8.1 0.11 1.07 \t test loss 32.93 0.29 1.84\n",
      "340 \t train loss 8.25 0.11 1.11 \t test loss 33.52 0.29 1.97\n",
      "341 \t train loss 8.33 0.11 1.09 \t test loss 33.85 0.29 1.86\n",
      "342 \t train loss 8.67 0.11 1.14 \t test loss 33.39 0.28 1.84\n",
      "343 \t train loss 8.42 0.11 1.07 \t test loss 33.36 0.28 1.86\n",
      "344 \t train loss 8.45 0.11 1.1 \t test loss 33.4 0.29 1.86\n",
      "345 \t train loss 8.33 0.12 1.08 \t test loss 32.5 0.3 1.82\n",
      "346 \t train loss 8.15 0.11 1.11 \t test loss 33.01 0.28 1.85\n",
      "347 \t train loss 8.37 0.11 1.07 \t test loss 31.88 0.28 1.84\n",
      "348 \t train loss 8.53 0.11 1.11 \t test loss 32.24 0.28 1.8\n",
      "349 \t train loss 8.7 0.11 1.09 \t test loss 33.64 0.28 1.8\n",
      "350 \t train loss 7.86 0.11 1.06 \t test loss 33.21 0.29 1.79\n",
      "351 \t train loss 8.72 0.12 1.14 \t test loss 32.72 0.3 1.77\n",
      "352 \t train loss 8.02 0.11 1.15 \t test loss 32.54 0.29 1.79\n",
      "353 \t train loss 7.98 0.11 1.11 \t test loss 32.6 0.28 1.79\n",
      "354 \t train loss 8.68 0.12 1.06 \t test loss 32.9 0.29 1.84\n",
      "355 \t train loss 8.12 0.11 1.12 \t test loss 32.84 0.28 1.74\n",
      "356 \t train loss 8.11 0.12 1.07 \t test loss 32.63 0.3 1.78\n",
      "357 \t train loss 8.32 0.11 1.09 \t test loss 33.58 0.29 1.72\n",
      "358 \t train loss 8.3 0.11 1.11 \t test loss 32.55 0.29 1.91\n",
      "359 \t train loss 8.31 0.12 1.13 \t test loss 32.56 0.3 1.77\n",
      "360 \t train loss 8.08 0.11 1.11 \t test loss 32.22 0.28 1.8\n",
      "361 \t train loss 8.46 0.11 1.2 \t test loss 31.88 0.29 1.72\n",
      "362 \t train loss 8.8 0.11 1.09 \t test loss 31.6 0.28 1.78\n",
      "363 \t train loss 8.12 0.11 1.09 \t test loss 32.21 0.28 1.8\n",
      "364 \t train loss 8.27 0.11 1.08 \t test loss 32.19 0.29 1.81\n",
      "365 \t train loss 9.0 0.11 1.18 \t test loss 32.86 0.28 1.8\n",
      "366 \t train loss 8.21 0.11 1.09 \t test loss 32.28 0.29 1.85\n",
      "367 \t train loss 8.41 0.11 1.14 \t test loss 33.15 0.29 1.82\n",
      "368 \t train loss 8.53 0.11 1.13 \t test loss 32.56 0.28 1.84\n",
      "369 \t train loss 7.93 0.11 1.07 \t test loss 32.81 0.3 1.81\n",
      "370 \t train loss 8.5 0.11 1.08 \t test loss 33.23 0.28 1.9\n",
      "371 \t train loss 8.28 0.11 1.07 \t test loss 32.48 0.28 1.85\n",
      "372 \t train loss 8.46 0.11 1.09 \t test loss 31.93 0.28 1.83\n",
      "373 \t train loss 8.03 0.11 1.13 \t test loss 32.38 0.29 1.83\n",
      "374 \t train loss 8.34 0.11 1.08 \t test loss 33.32 0.29 1.86\n",
      "375 \t train loss 8.55 0.11 1.1 \t test loss 33.64 0.29 1.79\n",
      "376 \t train loss 8.46 0.11 1.06 \t test loss 32.35 0.29 1.91\n",
      "377 \t train loss 8.67 0.11 1.08 \t test loss 32.42 0.28 1.82\n",
      "378 \t train loss 8.71 0.11 1.18 \t test loss 33.06 0.29 1.78\n",
      "379 \t train loss 7.93 0.11 1.15 \t test loss 32.6 0.29 1.77\n",
      "380 \t train loss 8.5 0.11 1.15 \t test loss 33.74 0.29 1.75\n",
      "381 \t train loss 8.39 0.11 1.1 \t test loss 32.41 0.28 1.77\n",
      "382 \t train loss 8.54 0.11 1.1 \t test loss 33.58 0.29 1.88\n",
      "383 \t train loss 8.25 0.11 1.14 \t test loss 33.15 0.28 1.85\n",
      "384 \t train loss 8.0 0.11 1.07 \t test loss 33.16 0.29 1.85\n",
      "385 \t train loss 8.09 0.11 1.09 \t test loss 32.54 0.28 1.84\n",
      "386 \t train loss 8.59 0.11 1.11 \t test loss 33.64 0.29 1.81\n",
      "387 \t train loss 8.12 0.12 1.12 \t test loss 32.42 0.3 1.78\n",
      "388 \t train loss 8.13 0.11 1.06 \t test loss 32.86 0.28 1.84\n",
      "389 \t train loss 8.29 0.11 1.13 \t test loss 32.85 0.29 1.85\n",
      "390 \t train loss 8.18 0.11 1.12 \t test loss 32.61 0.29 1.82\n",
      "391 \t train loss 7.97 0.11 1.1 \t test loss 32.22 0.28 1.81\n",
      "392 \t train loss 8.18 0.11 1.13 \t test loss 32.6 0.28 1.81\n",
      "393 \t train loss 7.87 0.11 1.1 \t test loss 33.25 0.3 1.83\n",
      "394 \t train loss 9.14 0.12 1.13 \t test loss 32.67 0.29 1.79\n",
      "395 \t train loss 8.35 0.11 1.15 \t test loss 33.61 0.29 1.76\n",
      "396 \t train loss 8.51 0.12 1.1 \t test loss 32.44 0.3 1.75\n",
      "397 \t train loss 8.39 0.12 1.08 \t test loss 32.67 0.28 1.79\n",
      "398 \t train loss 7.81 0.12 1.11 \t test loss 32.57 0.3 1.82\n",
      "399 \t train loss 8.01 0.11 1.06 \t test loss 32.21 0.28 1.85\n",
      "400 \t train loss 8.2 0.11 1.24 \t test loss 31.5 0.29 1.76\n",
      "401 \t train loss 7.91 0.11 1.13 \t test loss 31.65 0.29 1.81\n",
      "402 \t train loss 7.89 0.11 1.1 \t test loss 33.6 0.29 1.79\n",
      "403 \t train loss 8.09 0.11 1.13 \t test loss 32.08 0.29 1.77\n",
      "404 \t train loss 8.11 0.11 1.09 \t test loss 32.66 0.28 1.72\n",
      "405 \t train loss 7.77 0.11 1.1 \t test loss 32.46 0.29 1.85\n",
      "406 \t train loss 7.99 0.11 1.06 \t test loss 33.32 0.29 1.83\n",
      "407 \t train loss 8.15 0.1 1.04 \t test loss 32.53 0.29 1.78\n",
      "408 \t train loss 8.24 0.11 1.17 \t test loss 32.43 0.29 1.81\n",
      "409 \t train loss 8.37 0.11 1.08 \t test loss 32.49 0.29 1.83\n",
      "410 \t train loss 8.49 0.11 1.11 \t test loss 33.46 0.29 1.81\n",
      "411 \t train loss 8.22 0.11 1.1 \t test loss 31.39 0.29 1.8\n",
      "412 \t train loss 7.92 0.1 1.07 \t test loss 32.75 0.28 1.88\n",
      "413 \t train loss 7.7 0.11 1.04 \t test loss 32.01 0.29 1.74\n",
      "414 \t train loss 7.8 0.11 1.07 \t test loss 33.06 0.29 1.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415 \t train loss 8.05 0.11 1.14 \t test loss 32.83 0.28 1.82\n",
      "416 \t train loss 7.92 0.11 1.05 \t test loss 32.96 0.29 1.86\n",
      "417 \t train loss 7.83 0.11 1.1 \t test loss 33.61 0.29 1.78\n",
      "418 \t train loss 8.27 0.11 1.06 \t test loss 32.08 0.29 1.83\n",
      "419 \t train loss 7.93 0.11 1.09 \t test loss 32.49 0.29 1.79\n",
      "420 \t train loss 8.12 0.1 1.1 \t test loss 32.56 0.28 1.76\n",
      "421 \t train loss 8.03 0.11 1.14 \t test loss 32.78 0.29 1.76\n",
      "422 \t train loss 7.77 0.11 1.08 \t test loss 32.28 0.3 1.77\n",
      "423 \t train loss 7.68 0.11 1.09 \t test loss 33.01 0.29 1.8\n",
      "424 \t train loss 7.93 0.1 1.08 \t test loss 31.41 0.28 1.73\n",
      "425 \t train loss 7.68 0.1 1.05 \t test loss 32.75 0.28 1.82\n",
      "426 \t train loss 7.88 0.11 1.06 \t test loss 31.97 0.29 1.77\n",
      "427 \t train loss 7.86 0.11 1.1 \t test loss 31.88 0.29 1.71\n",
      "428 \t train loss 7.47 0.11 1.03 \t test loss 32.31 0.28 1.84\n",
      "429 \t train loss 8.06 0.11 1.08 \t test loss 33.32 0.28 1.83\n",
      "430 \t train loss 7.94 0.1 1.08 \t test loss 33.06 0.28 1.95\n",
      "431 \t train loss 7.67 0.11 1.15 \t test loss 33.36 0.28 1.8\n",
      "432 \t train loss 7.82 0.11 1.13 \t test loss 33.52 0.29 1.86\n",
      "433 \t train loss 7.48 0.11 1.11 \t test loss 32.33 0.29 1.82\n",
      "434 \t train loss 7.53 0.11 1.06 \t test loss 32.47 0.28 1.85\n",
      "435 \t train loss 7.72 0.1 1.08 \t test loss 32.83 0.29 1.8\n",
      "436 \t train loss 8.04 0.11 1.1 \t test loss 32.23 0.28 1.84\n",
      "437 \t train loss 7.9 0.11 1.1 \t test loss 32.34 0.29 1.85\n",
      "438 \t train loss 8.44 0.11 1.12 \t test loss 32.16 0.29 1.8\n",
      "439 \t train loss 7.89 0.11 1.08 \t test loss 32.91 0.29 1.8\n",
      "440 \t train loss 8.41 0.11 1.07 \t test loss 31.91 0.29 1.84\n",
      "441 \t train loss 7.63 0.1 1.07 \t test loss 31.08 0.29 1.82\n",
      "442 \t train loss 8.0 0.11 1.09 \t test loss 32.74 0.28 1.86\n",
      "443 \t train loss 7.78 0.1 1.08 \t test loss 32.13 0.28 1.83\n",
      "444 \t train loss 7.88 0.11 1.07 \t test loss 32.18 0.29 1.86\n",
      "445 \t train loss 7.88 0.11 1.12 \t test loss 32.69 0.28 1.85\n",
      "446 \t train loss 7.64 0.11 1.08 \t test loss 32.55 0.28 1.89\n",
      "447 \t train loss 7.82 0.1 1.05 \t test loss 32.6 0.29 1.73\n",
      "448 \t train loss 7.57 0.11 1.09 \t test loss 31.94 0.29 1.79\n",
      "449 \t train loss 7.79 0.1 1.07 \t test loss 32.95 0.29 1.76\n",
      "450 \t train loss 7.72 0.1 1.09 \t test loss 32.52 0.28 1.78\n",
      "451 \t train loss 7.68 0.11 1.08 \t test loss 32.22 0.29 1.8\n",
      "452 \t train loss 7.32 0.11 1.11 \t test loss 32.7 0.29 1.82\n",
      "453 \t train loss 8.02 0.1 1.07 \t test loss 33.79 0.29 1.88\n",
      "454 \t train loss 8.03 0.11 1.13 \t test loss 32.99 0.3 1.82\n",
      "455 \t train loss 8.06 0.11 1.07 \t test loss 31.3 0.28 1.82\n",
      "456 \t train loss 7.72 0.1 1.07 \t test loss 32.04 0.28 1.78\n",
      "457 \t train loss 7.55 0.11 1.15 \t test loss 33.39 0.29 1.8\n",
      "458 \t train loss 7.56 0.11 1.11 \t test loss 32.55 0.28 1.72\n",
      "459 \t train loss 7.83 0.11 1.15 \t test loss 31.84 0.3 1.72\n",
      "460 \t train loss 7.53 0.11 1.11 \t test loss 33.08 0.28 1.84\n",
      "461 \t train loss 7.7 0.11 1.15 \t test loss 32.16 0.28 1.83\n",
      "462 \t train loss 7.49 0.11 1.08 \t test loss 33.65 0.28 1.81\n",
      "463 \t train loss 7.5 0.1 1.1 \t test loss 33.08 0.29 1.83\n",
      "464 \t train loss 7.7 0.11 1.06 \t test loss 32.43 0.28 1.81\n",
      "465 \t train loss 7.97 0.1 1.1 \t test loss 32.65 0.29 1.81\n",
      "466 \t train loss 7.52 0.11 1.12 \t test loss 32.34 0.29 1.79\n",
      "467 \t train loss 7.57 0.1 1.06 \t test loss 32.38 0.28 1.87\n",
      "468 \t train loss 7.69 0.11 1.15 \t test loss 32.73 0.29 1.9\n",
      "469 \t train loss 8.08 0.11 1.05 \t test loss 32.95 0.28 1.87\n",
      "470 \t train loss 7.71 0.1 1.11 \t test loss 32.85 0.28 1.81\n",
      "471 \t train loss 7.98 0.1 1.05 \t test loss 33.13 0.28 1.74\n",
      "472 \t train loss 7.78 0.1 1.07 \t test loss 33.02 0.28 1.78\n",
      "473 \t train loss 7.55 0.1 1.17 \t test loss 33.08 0.28 1.78\n",
      "474 \t train loss 7.93 0.1 1.04 \t test loss 32.58 0.29 1.83\n",
      "475 \t train loss 7.76 0.11 1.07 \t test loss 32.81 0.28 1.82\n",
      "476 \t train loss 8.09 0.12 1.14 \t test loss 32.8 0.3 1.78\n",
      "477 \t train loss 7.79 0.1 1.16 \t test loss 33.49 0.28 1.79\n",
      "478 \t train loss 7.68 0.11 1.07 \t test loss 32.57 0.3 1.82\n",
      "479 \t train loss 7.75 0.11 1.04 \t test loss 32.33 0.29 1.85\n",
      "480 \t train loss 8.16 0.11 1.1 \t test loss 32.36 0.29 1.87\n",
      "481 \t train loss 8.11 0.1 1.06 \t test loss 33.0 0.29 1.85\n",
      "482 \t train loss 7.65 0.11 1.12 \t test loss 33.03 0.29 1.81\n",
      "483 \t train loss 7.51 0.1 1.09 \t test loss 32.59 0.29 1.84\n",
      "484 \t train loss 8.03 0.11 1.08 \t test loss 32.2 0.3 1.87\n",
      "485 \t train loss 7.69 0.1 1.04 \t test loss 33.09 0.28 1.82\n",
      "486 \t train loss 7.6 0.11 1.07 \t test loss 31.84 0.29 1.8\n",
      "487 \t train loss 7.72 0.11 1.11 \t test loss 33.15 0.29 1.97\n",
      "488 \t train loss 7.53 0.1 1.07 \t test loss 32.71 0.28 1.88\n",
      "489 \t train loss 8.14 0.1 1.06 \t test loss 32.73 0.28 1.87\n",
      "490 \t train loss 7.89 0.1 1.11 \t test loss 33.18 0.29 1.96\n",
      "491 \t train loss 7.64 0.11 1.04 \t test loss 32.29 0.29 1.79\n",
      "492 \t train loss 7.6 0.11 1.05 \t test loss 32.54 0.29 1.85\n",
      "493 \t train loss 7.61 0.1 1.08 \t test loss 32.21 0.29 1.86\n",
      "494 \t train loss 7.7 0.11 1.07 \t test loss 33.59 0.29 1.76\n",
      "495 \t train loss 7.53 0.1 1.13 \t test loss 33.02 0.29 1.78\n",
      "496 \t train loss 7.85 0.11 1.06 \t test loss 33.67 0.28 1.76\n",
      "497 \t train loss 7.99 0.11 1.07 \t test loss 31.83 0.29 1.81\n",
      "498 \t train loss 7.45 0.12 1.08 \t test loss 32.57 0.3 1.9\n",
      "499 \t train loss 7.39 0.1 1.08 \t test loss 32.12 0.28 1.78\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(500):  # loop over the dataset multiple times\n",
    "    optimizer = optim.Adam(net.parameters(), lr=step_decay(epoch))\n",
    "    for i, data in enumerate(ds, 0):\n",
    "        # get the inputs\n",
    "        batch_inputs, batch_orient, batch_dim, batch_pos = data\n",
    "        (batch_inputs, batch_orient, batch_dim, batch_pos) = (batch_inputs.to(device), \n",
    "                                                                    batch_orient.to(device),\n",
    "                                                                    batch_dim.to(device),\n",
    "                                                                    batch_pos.to(device))\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        batch_outputs = net(batch_inputs)\n",
    "        out_orient, out_dim, out_pos = batch_outputs[:,:2], batch_outputs[:,2:5], batch_outputs[:,5:8]\n",
    "        \n",
    "        l_orient = sin_cos_LossFunc(out_orient, batch_orient)\n",
    "        l_dim = dim_LossFunc(out_dim, batch_dim)\n",
    "        l_pos = pos_LossFunc(out_pos, batch_pos)\n",
    "        \n",
    "        l1.append(l_orient)\n",
    "        l2.append(l_dim)\n",
    "        l3.append(l_pos)\n",
    "        \n",
    "        loss = p1*l_orient + p2*l_dim + p3*l_pos\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #print(find_loss(train_inputs, train_gts, net))\n",
    "    \n",
    "    train_loss = find_losses(train_inputs, train_angles, train_dim, train_pos, net)\n",
    "    train_losses['angle loss'].append(np.mean(train_loss[1]))\n",
    "    train_losses['dim loss'].append(np.mean(train_loss[2]))\n",
    "    train_losses['pos loss'].append(np.mean(train_loss[3]))\n",
    "    \n",
    "    test_loss = find_losses(test_inputs, test_angles, test_dim, test_pos, net)\n",
    "    test_losses['angle loss'].append(np.mean(test_loss[1]))\n",
    "    test_losses['dim loss'].append(np.mean(test_loss[2]))\n",
    "    test_losses['pos loss'].append(np.mean(test_loss[3]))\n",
    "    \n",
    "    print(epoch,\n",
    "          '\\t train loss',\n",
    "          round(np.mean(train_loss[1]),2),\n",
    "          round(np.mean(train_loss[2]),2),\n",
    "          round(np.mean(train_loss[3]),2),\n",
    "          '\\t test loss',\n",
    "          round(np.mean(test_loss[1]),2),\n",
    "          round(np.mean(test_loss[2]),2),\n",
    "          round(np.mean(test_loss[3]),2))\n",
    "    \n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import gaussian_filter1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls1=[p1*x.cpu().detach().numpy() for x in l1]\n",
    "ls2=[p2*x.cpu().detach().numpy() for x in l2]\n",
    "ls3=[p3*x.cpu().detach().numpy() for x in l3]\n",
    "\n",
    "plt.plot(gaussian_filter1d(ls1[100:], sigma=200))\n",
    "plt.plot(gaussian_filter1d(ls2[100:], sigma=200))\n",
    "plt.plot(gaussian_filter1d(ls3[100:], sigma=300))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gaussian_filter1d([1.0*x for x in train_losses['angle loss']], sigma=3))\n",
    "plt.plot(gaussian_filter1d([100*x for x in train_losses['dim loss']], sigma=3))\n",
    "plt.plot(gaussian_filter1d([8*x for x in train_losses['pos loss']], sigma=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'net-kitti-multi.pkl'\n",
    "torch.save(net.state_dict(), name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.load('net-nuscunes-multi.pkl')\n",
    "net.load_state_dict(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2003\n",
    "out_sample = infer(test_inputs, net)\n",
    "print(np.round(test_angles[i],2), np.round(test_dim[i],2), np.round(test_pos[i],2))\n",
    "print(np.round(out_sample[0][i],2), np.round(out_sample[1][i],2), np.round(out_sample[2][i],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': '003455',\n",
       " 'truncation': 0.0,\n",
       " 'occlusion': 0,\n",
       " 'obs_angle': 257.92226013779197,\n",
       " 'gt_bbox': [731.02, 166.17, 753.54, 222.42],\n",
       " 'gt_dim': [1.83, 0.77, 1.01],\n",
       " 'gt_pos': [4.33, 1.62, 24.04],\n",
       " 'abs_orient': 177.66254265501595,\n",
       " 'r_angle': 10.210437543659532,\n",
       " 'gt_orient': 167.45210511135642,\n",
       " 'keypoints': array([[734.4 , 173.2 ],\n",
       "        [733.2 , 172.35],\n",
       "        [741.5 , 171.7 ],\n",
       "        [737.7 , 172.35],\n",
       "        [742.45, 172.45],\n",
       "        [736.05, 179.05],\n",
       "        [744.8 , 179.5 ],\n",
       "        [730.85, 187.1 ],\n",
       "        [749.7 , 185.75],\n",
       "        [728.45, 194.05],\n",
       "        [750.6 , 191.45],\n",
       "        [738.05, 196.  ],\n",
       "        [742.6 , 195.6 ],\n",
       "        [737.15, 206.6 ],\n",
       "        [741.95, 206.45],\n",
       "        [737.05, 218.35],\n",
       "        [741.  , 218.  ]]),\n",
       " 'pp_box': [726.2431228756905,\n",
       "  167.0319797515869,\n",
       "  753.0128299057484,\n",
       "  223.50798914909362]}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_sample = infer(test_inputs, net)\n",
    "for i, data in enumerate(test_data):\n",
    "    data['pred_orient'] = out_sample[0][i]\n",
    "    data['pred_dim'] = out_sample[1][i]\n",
    "    data['pred_pos'] = out_sample[2][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category(pedestrian):\n",
    "    \n",
    "    box = pedestrian['gt_bbox']\n",
    "    \n",
    "    trunc = pedestrian['truncation']\n",
    "    \n",
    "    occ = pedestrian['occlusion']\n",
    "\n",
    "    hh = box[3] - box[1]\n",
    "\n",
    "    if hh >= 40 and trunc <= 0.15 and occ <= 0:\n",
    "        cat = 'easy'\n",
    "    elif trunc <= 0.3 and occ <= 1:\n",
    "        cat = 'moderate'\n",
    "    else:\n",
    "        cat = 'hard'\n",
    "\n",
    "    return cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in test_data:\n",
    "    data['cat'] = get_category(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_df = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abs_orient</th>\n",
       "      <th>gt_orient</th>\n",
       "      <th>obs_angle</th>\n",
       "      <th>occlusion</th>\n",
       "      <th>pred_orient</th>\n",
       "      <th>r_angle</th>\n",
       "      <th>truncation</th>\n",
       "      <th>error_orient</th>\n",
       "      <th>error_dim</th>\n",
       "      <th>error_pos</th>\n",
       "      <th>error_x</th>\n",
       "      <th>error_y</th>\n",
       "      <th>error_z</th>\n",
       "      <th>error_w</th>\n",
       "      <th>error_d</th>\n",
       "      <th>error_h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1798.000000</td>\n",
       "      <td>1798.000000</td>\n",
       "      <td>1798.000000</td>\n",
       "      <td>1798.000000</td>\n",
       "      <td>1798.000000</td>\n",
       "      <td>1798.000000</td>\n",
       "      <td>1798.000000</td>\n",
       "      <td>1798.000000</td>\n",
       "      <td>1798.000000</td>\n",
       "      <td>1798.000000</td>\n",
       "      <td>1798.000000</td>\n",
       "      <td>1798.000000</td>\n",
       "      <td>1798.000000</td>\n",
       "      <td>1798.000000</td>\n",
       "      <td>1798.000000</td>\n",
       "      <td>1798.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>177.443037</td>\n",
       "      <td>210.018273</td>\n",
       "      <td>170.037609</td>\n",
       "      <td>0.478865</td>\n",
       "      <td>181.437220</td>\n",
       "      <td>1.062139</td>\n",
       "      <td>0.004761</td>\n",
       "      <td>21.646925</td>\n",
       "      <td>1.408260</td>\n",
       "      <td>6.396465</td>\n",
       "      <td>1.201076</td>\n",
       "      <td>0.809655</td>\n",
       "      <td>6.083713</td>\n",
       "      <td>1.086124</td>\n",
       "      <td>0.140993</td>\n",
       "      <td>0.858163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>116.381856</td>\n",
       "      <td>109.979948</td>\n",
       "      <td>104.789532</td>\n",
       "      <td>0.700066</td>\n",
       "      <td>109.379693</td>\n",
       "      <td>17.642716</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>30.252247</td>\n",
       "      <td>0.138802</td>\n",
       "      <td>4.397313</td>\n",
       "      <td>1.349580</td>\n",
       "      <td>0.280565</td>\n",
       "      <td>4.386612</td>\n",
       "      <td>0.096515</td>\n",
       "      <td>0.102092</td>\n",
       "      <td>0.216248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.045626</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>0.664210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269359</td>\n",
       "      <td>-39.360874</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007569</td>\n",
       "      <td>0.950898</td>\n",
       "      <td>0.524527</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>0.015170</td>\n",
       "      <td>0.768051</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.030577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>146.973826</td>\n",
       "      <td>77.440555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>104.585445</td>\n",
       "      <td>-11.593829</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.368639</td>\n",
       "      <td>1.322625</td>\n",
       "      <td>2.723681</td>\n",
       "      <td>0.322901</td>\n",
       "      <td>0.652914</td>\n",
       "      <td>2.433407</td>\n",
       "      <td>1.026101</td>\n",
       "      <td>0.057891</td>\n",
       "      <td>0.738216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>177.089585</td>\n",
       "      <td>200.771185</td>\n",
       "      <td>162.524787</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>177.056713</td>\n",
       "      <td>1.121764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.670379</td>\n",
       "      <td>1.389222</td>\n",
       "      <td>5.519494</td>\n",
       "      <td>0.754712</td>\n",
       "      <td>0.802232</td>\n",
       "      <td>5.299237</td>\n",
       "      <td>1.085181</td>\n",
       "      <td>0.125188</td>\n",
       "      <td>0.833857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>271.237168</td>\n",
       "      <td>321.780800</td>\n",
       "      <td>258.495218</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>273.687449</td>\n",
       "      <td>14.093394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.878794</td>\n",
       "      <td>1.468841</td>\n",
       "      <td>8.877796</td>\n",
       "      <td>1.597467</td>\n",
       "      <td>0.951811</td>\n",
       "      <td>8.644560</td>\n",
       "      <td>1.145324</td>\n",
       "      <td>0.204192</td>\n",
       "      <td>0.955203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>359.472668</td>\n",
       "      <td>359.967402</td>\n",
       "      <td>359.908748</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>359.774842</td>\n",
       "      <td>39.629724</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>179.988820</td>\n",
       "      <td>1.864516</td>\n",
       "      <td>24.943258</td>\n",
       "      <td>10.105019</td>\n",
       "      <td>2.199445</td>\n",
       "      <td>23.466716</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.578856</td>\n",
       "      <td>1.525564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        abs_orient    gt_orient    obs_angle    occlusion  pred_orient  \\\n",
       "count  1798.000000  1798.000000  1798.000000  1798.000000  1798.000000   \n",
       "mean    177.443037   210.018273   170.037609     0.478865   181.437220   \n",
       "std     116.381856   109.979948   104.789532     0.700066   109.379693   \n",
       "min       0.045626     0.002583     0.664210     0.000000     0.269359   \n",
       "25%      90.000000   146.973826    77.440555     0.000000   104.585445   \n",
       "50%     177.089585   200.771185   162.524787     0.000000   177.056713   \n",
       "75%     271.237168   321.780800   258.495218     1.000000   273.687449   \n",
       "max     359.472668   359.967402   359.908748     3.000000   359.774842   \n",
       "\n",
       "           r_angle   truncation  error_orient    error_dim    error_pos  \\\n",
       "count  1798.000000  1798.000000   1798.000000  1798.000000  1798.000000   \n",
       "mean      1.062139     0.004761     21.646925     1.408260     6.396465   \n",
       "std      17.642716     0.023200     30.252247     0.138802     4.397313   \n",
       "min     -39.360874     0.000000      0.007569     0.950898     0.524527   \n",
       "25%     -11.593829     0.000000      5.368639     1.322625     2.723681   \n",
       "50%       1.121764     0.000000     12.670379     1.389222     5.519494   \n",
       "75%      14.093394     0.000000     23.878794     1.468841     8.877796   \n",
       "max      39.629724     0.190000    179.988820     1.864516    24.943258   \n",
       "\n",
       "           error_x      error_y      error_z      error_w      error_d  \\\n",
       "count  1798.000000  1798.000000  1798.000000  1798.000000  1798.000000   \n",
       "mean      1.201076     0.809655     6.083713     1.086124     0.140993   \n",
       "std       1.349580     0.280565     4.386612     0.096515     0.102092   \n",
       "min       0.000168     0.003542     0.015170     0.768051     0.000073   \n",
       "25%       0.322901     0.652914     2.433407     1.026101     0.057891   \n",
       "50%       0.754712     0.802232     5.299237     1.085181     0.125188   \n",
       "75%       1.597467     0.951811     8.644560     1.145324     0.204192   \n",
       "max      10.105019     2.199445    23.466716     1.386294     0.578856   \n",
       "\n",
       "           error_h  \n",
       "count  1798.000000  \n",
       "mean      0.858163  \n",
       "std       0.216248  \n",
       "min       0.030577  \n",
       "25%       0.738216  \n",
       "50%       0.833857  \n",
       "75%       0.955203  \n",
       "max       1.525564  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  add orientation error\n",
    "angle_loss = abs(test_df['gt_orient']-test_df['pred_orient'])\n",
    "angle_loss = np.array(list(map(lambda x: x if x < 180 else 360-x, angle_loss)))\n",
    "test_df['error_orient'] = angle_loss\n",
    "\n",
    "## add dim error\n",
    "pred_dim = np.array(test_df['pred_dim'].tolist())\n",
    "og_dim = np.array(test_df['gt_dim'].tolist())\n",
    "\n",
    "dim_loss = np.linalg.norm(pred_dim - og_dim, axis=1)\n",
    "test_df['error_dim'] = dim_loss\n",
    "\n",
    "## add pred pos error\n",
    "pred_pos = np.array(test_df['pred_pos'].tolist())\n",
    "og_pos = np.array(test_df['gt_pos'].tolist())\n",
    "\n",
    "pos_loss = np.linalg.norm(pred_pos - og_pos, axis=1)\n",
    "test_df['error_pos'] = pos_loss\n",
    "\n",
    "## add naive pos error\n",
    "\"\"\"pred_pos = np.array(test_df['naive_pos'].tolist())\n",
    "og_pos = np.array(test_df['position'].tolist())\n",
    "\n",
    "pos_loss = np.linalg.norm(pred_pos - og_pos, axis=1)\n",
    "test_df['error_npos'] = pos_loss\n",
    "\"\"\"\n",
    "## add xyz error\n",
    "test_df['error_x'] = abs(pred_pos[:,0] - og_pos[:,0])\n",
    "test_df['error_y'] = abs(pred_pos[:,1] - og_pos[:,1])\n",
    "test_df['error_z'] = abs(pred_pos[:,2] - og_pos[:,2])\n",
    "\n",
    "test_df['error_w'] = abs(pred_dim[:,0] - og_dim[:,0])\n",
    "test_df['error_d'] = abs(pred_dim[:,1] - og_dim[:,1])\n",
    "test_df['error_h'] = abs(pred_dim[:,2] - og_dim[:,2])\n",
    "\n",
    "\n",
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
